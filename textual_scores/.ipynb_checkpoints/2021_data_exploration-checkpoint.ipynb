{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 12:32:03.722064: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import time\n",
    "import scipy\n",
    "import string \n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "CUDA_VISIBLE_DEVICES=2\n",
    "CUDA_LAUNCH_BLOCKING=1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_url</th>\n",
       "      <th>annotations_short_term</th>\n",
       "      <th>annotations_long_term</th>\n",
       "      <th>scores_raw_short_term</th>\n",
       "      <th>scores_raw_long_term</th>\n",
       "      <th>scores_normalized_short_term</th>\n",
       "      <th>decay_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>https://mtc.cdn.vine.co/r/videos_h264high/9EB0...</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.624240</td>\n",
       "      <td>-1.289843e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>https://mtc.cdn.vine.co/r/videos_h264high/A8B3...</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>https://mtc.cdn.vine.co/r/videos/267829AEFA128...</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.672876</td>\n",
       "      <td>-1.279030e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>https://mtc.cdn.vine.co/r/videos_h264high/B974...</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>https://mtc.cdn.vine.co/r/videos_h264high/C4D6...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.769838</td>\n",
       "      <td>-1.163618e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>6136</td>\n",
       "      <td>https://mtc.cdn.vine.co/r/videos_h264high/2D38...</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>6154</td>\n",
       "      <td>https://v.cdn.vine.co/r/videos/09E48CC7D810160...</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>6177</td>\n",
       "      <td>https://mtc.cdn.vine.co/r/videos_h264high/12B3...</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>6182</td>\n",
       "      <td>https://mtc.cdn.vine.co/r/videos/AA485CE97B136...</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.431271e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>6191</td>\n",
       "      <td>https://mtc.cdn.vine.co/r/videos/36D3C8CB78120...</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.697797</td>\n",
       "      <td>-2.918228e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     video_id                                          video_url  \\\n",
       "0           8  https://mtc.cdn.vine.co/r/videos_h264high/9EB0...   \n",
       "1          26  https://mtc.cdn.vine.co/r/videos_h264high/A8B3...   \n",
       "2          33  https://mtc.cdn.vine.co/r/videos/267829AEFA128...   \n",
       "3          46  https://mtc.cdn.vine.co/r/videos_h264high/B974...   \n",
       "4          64  https://mtc.cdn.vine.co/r/videos_h264high/C4D6...   \n",
       "..        ...                                                ...   \n",
       "583      6136  https://mtc.cdn.vine.co/r/videos_h264high/2D38...   \n",
       "584      6154  https://v.cdn.vine.co/r/videos/09E48CC7D810160...   \n",
       "585      6177  https://mtc.cdn.vine.co/r/videos_h264high/12B3...   \n",
       "586      6182  https://mtc.cdn.vine.co/r/videos/AA485CE97B136...   \n",
       "587      6191  https://mtc.cdn.vine.co/r/videos/36D3C8CB78120...   \n",
       "\n",
       "     annotations_short_term  annotations_long_term  scores_raw_short_term  \\\n",
       "0                        20                      7                   0.75   \n",
       "1                        23                      7                   0.87   \n",
       "2                        26                      4                   0.69   \n",
       "3                        23                      7                   0.87   \n",
       "4                        19                      9                   0.84   \n",
       "..                      ...                    ...                    ...   \n",
       "583                      20                      7                   0.90   \n",
       "584                      21                      9                   0.90   \n",
       "585                      23                      6                   0.96   \n",
       "586                      17                      6                   1.00   \n",
       "587                      20                      7                   0.70   \n",
       "\n",
       "     scores_raw_long_term  scores_normalized_short_term   decay_alpha  \n",
       "0                    0.57                      0.624240 -1.289843e-02  \n",
       "1                    0.43                      0.869565  0.000000e+00  \n",
       "2                    0.75                      0.672876 -1.279030e-03  \n",
       "3                    0.57                      0.869565  0.000000e+00  \n",
       "4                    0.56                      0.769838 -1.163618e-02  \n",
       "..                    ...                           ...           ...  \n",
       "583                  1.00                      0.900000  0.000000e+00  \n",
       "584                  1.00                      0.904762  0.000000e+00  \n",
       "585                  0.83                      0.956522  0.000000e+00  \n",
       "586                  0.67                      1.000000 -1.431271e-08  \n",
       "587                  0.86                      0.697797 -2.918228e-04  \n",
       "\n",
       "[588 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores=pd.read_csv('data_2021/TRECVid Data/training_set/train_scores.csv')\n",
    "#df_scores=pd.read_csv('data_2021/Memento10k Data/training_set/train_scores.csv')\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text=pd.read_csv('data_2021/TRECVid Data/training_set/train_text_descriptions.csv')\n",
    "\n",
    "\n",
    "\n",
    "#df_text=pd.read_csv('data_2021/Memento10k Data/training_set/train_text_descriptions.csv')\n",
    "#df_text['description']=df_text['description_0']+df_text['description_1']+df_text['description_2']+df_text['description_3']+df_text['description_4']\n",
    "\n",
    "#train_text_descriptions=df.groupby('id').agg(lambda x: x.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_descriptions=pd.read_csv('data_2021/TRECVid Data/test_set/test_text_descriptions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "spearman = lambda x,y: spearmanr(x, y).correlation\n",
    "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\"]\n",
    "\n",
    "def tokenize(s):\n",
    "    numbers = {'2': 'two', '3': 'three', '4': 'four'}\n",
    "    s = ''.join(c for c in s if c not in string.punctuation or c == ' ').lower()\n",
    "    t = RegexpTokenizer(r'\\w+').tokenize(s)\n",
    "    t = [lemmatizer.lemmatize(w) if w not in numbers else numbers[w] for w in t if w not in stopwords]\n",
    "    return ' '.join(t)\n",
    "\n",
    "def tokenize2(s): # keep stopwords and don't lemmatize\n",
    "    numbers = {'2': 'two', '3': 'three', '4': 'four'}\n",
    "    s = ''.join(c for c in s if c not in string.punctuation or c == ' ').lower()\n",
    "    t = RegexpTokenizer(r'\\w+').tokenize(s)\n",
    "    t = [w if w not in numbers else numbers[w] for w in t]\n",
    "    return ' '.join(t)\n",
    "\n",
    "def tokenize3(s): # remove duplicates\n",
    "    numbers = {'2': 'two', '3': 'three', '4': 'four'}\n",
    "    s = ''.join(c for c in s if c not in string.punctuation or c == ' ').lower()\n",
    "    t = RegexpTokenizer(r'\\w+').tokenize(s)\n",
    "    t = [w if w not in numbers else numbers[w] for w in t if w not in stopwords]\n",
    "    return ' '.join(set(t))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# For videos having multiple descriptions\n",
    "text_concat = df_text[['video_id','description']].groupby(['video_id'])['description'].transform(lambda x: ' '.join(x)).drop_duplicates()\n",
    "\n",
    "#deep_captions = [ l[8:].strip() for l in open(args.deep_caption_path)]\n",
    "\n",
    "df_data = df_scores.copy()\n",
    "df_data['text'] = text_concat.values\n",
    "df_data['short_term'] = df_data['scores_raw_short_term']\n",
    "if 'scores_raw_long_term' in df_data.columns:\n",
    "    df_data['long_term'] = df_data['scores_raw_long_term']\n",
    "#df_data['deep_caption'] = deep_captions[:590]\n",
    "#df_data = df_data[['video_id', 'text', 'deep_caption', 'short_term', 'long_term']]\n",
    "if 'long_term' in df_data.columns:\n",
    "    df_data = df_data[['video_id', 'text', 'short_term', 'long_term']]\n",
    "else:\n",
    "    df_data = df_data[['video_id', 'text', 'short_term']]\n",
    "    \n",
    "df_data['content'] = df_data['text'] #+ '  ' + df_data['deep_caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda'\n",
    "model_id = 'gpt2-large'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8015ae931014513beb298bba6d836e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/588 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity_sentence=[]\n",
    "for i in tqdm(df_data.index):\n",
    "    encodings = tokenizer(df_data['content'][i], return_tensors='pt')\n",
    "\n",
    "    input_ids=encodings.input_ids.to(device)\n",
    "    \n",
    "    \n",
    "    df_data['content'][i]\n",
    "    target_ids = input_ids\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        neg_log_likelihood = outputs[0].item()/len(df_data['content'][i])\n",
    "        #print(neg_log_likelihood)\n",
    "\n",
    "    perplexity_sentence.append(neg_log_likelihood)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0510308929416588\n"
     ]
    }
   ],
   "source": [
    "#print(perplexity_sentence)\n",
    "df_data['perplexity']=perplexity_sentence\n",
    "column='perplexity'\n",
    "df_data[column] = df_data[column] /df_data[column].abs().max()\n",
    "df_data['perplexity'].to_csv('perplexity_norm.csv')\n",
    "#print(df_data.sort_values(by=['perplexity']))\n",
    "\n",
    "#print(df_data['perplexity'].to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(spearmanr(np.array(df_data['perplexity'].to_numpy()), df_data['long_term'].to_numpy()).correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd59eae6c324df29bc3ef721fa299dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/588 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#classifier = pipeline(\"zero-shot-classification\", device=2,multiclass=True) # to utilize GPU\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"joeddav/bart-large-mnli-yahoo-answers\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"joeddav/bart-large-mnli-yahoo-answers\")\n",
    "classifier = pipeline(\"zero-shot-classification\",device=2,model=model, tokenizer=tokenizer,multi_label=True) # to utilize GPU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "candidate_labels=['ball sport','landscape','person','people','violent','accident','dance','animal','self-defeating','sarcasm','smile','shout','girl','falling','punch','face','twerk','slap','funny','unusual','surprising']\n",
    "classification_scores=[]\n",
    "for i in tqdm(df_data.index):\n",
    "    results=classifier(df_data['content'][i], candidate_labels)\n",
    "    results_score = dict(zip(results['labels'], results['scores']))\n",
    "    classification_scores.append(results_score)\n",
    "    #print(classification_scores)\n",
    "\n",
    " #['violent','accident','self-defeating','sarcasm','humor','salient','funny','unusual']  \n",
    "\n",
    "df_classification=pd.DataFrame(classification_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07226961868144992\n",
      "-0.034461578943553055\n"
     ]
    }
   ],
   "source": [
    "label=['chocking']\n",
    "print(spearmanr(np.array(df_classification[label].to_numpy()), df_data['short_term'].to_numpy()).correlation)\n",
    "print(spearmanr(np.array(df_classification[label].to_numpy()), df_data['long_term'].to_numpy()).correlation)\n",
    "df_classification[['violent','funny','weird','chocking','cute','cool']].to_csv('funny.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_classification.head())\n",
    "X_classification=df_classification.to_numpy()\n",
    "print(X_classification)\n",
    "X_classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_data.text.values\n",
    "corpus_tokenized = [tokenize3(s) for s in corpus]\n",
    "#corpus_tokenized2 = [tokenize2(s) for s in corpus]\n",
    "#corpus_tokenized3 = [tokenize3(s) for s in corpus]\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=4, stop_words='english', ngram_range=(1, 2))\n",
    "cv=vectorizer.fit(corpus_tokenized)\n",
    "print(list(cv.vocabulary_.keys())[:30])\n",
    "\n",
    "X_train = vectorizer.transform(corpus)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train.toarray())\n",
    "#w2v = pickle.load(open('/data/glove.6B/glove.w2v.6B.300d.pickle', 'rb'))\n",
    "\n",
    "\n",
    "X_tfidf = []\n",
    "X_w2v   = []\n",
    "Y = []\n",
    "\n",
    "\n",
    "for i, entry in df_data.iterrows():\n",
    "    text = tokenize(entry['text'])\n",
    "    if 'long_term' in df_data.columns:\n",
    "        y = (entry['short_term'], entry['long_term'])\n",
    "    else:\n",
    "        y = (entry['short_term'],'no_lt')\n",
    "        \n",
    "    \n",
    "    x_tfidf = vectorizer.transform([text]).toarray()[0]\n",
    "    #words = [word for word in text.split(' ') if word in w2v]\n",
    "    #x_w2v = np.zeros([300]) if not words else np.mean([w2v[word] for word in words], axis=0)\n",
    "\n",
    "    \n",
    "    X_tfidf.append(x_tfidf)\n",
    "    #X_w2v.append(x_w2v)\n",
    "    Y.append(y)\n",
    "\n",
    "X_tfidf = np.array(X_tfidf)\n",
    "#X_w2v = np.array(X_w2v)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(X_tfidf.shape, Y.shape)\n",
    "X_classification.shape,Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def enumerate_models(models):\n",
    "    instances = []\n",
    "    for model_name, (model, hyperparameters) in models.items():\n",
    "        configs = {}\n",
    "        if len(hyperparameters) > 0:\n",
    "            params, vals = list(hyperparameters.keys()), list(hyperparameters.values())\n",
    "            configs = [dict(zip(params, vv)) for vv in list(itertools.product(*vals))]\n",
    "            for config in configs:\n",
    "                m = model(**config)\n",
    "                instances.append(m)\n",
    "        else:\n",
    "            instances.append(model())\n",
    "    return instances\n",
    "\n",
    "\n",
    "regression_models = {\n",
    "    # 'LogisticRegression': (LogisticRegression, {\"C\": [1e3, 1, 1e-3], \"penalty\": ['l1', 'l2', 'elasticnet']}),\n",
    "    # 'LinearRegression': (LinearRegression, {}),\n",
    "    # 'MLPRegressor': (MLPRegressor, {'alpha': [1e-3,  1e-7], 'hidden_layer_sizes': [(10,), (100,)]}), # 1e-5,, (50,), \n",
    "    # 'SGDRegressor': (SGDRegressor, {'alpha': [0.0001, 0.1,]}),\n",
    "    # 'SVR': (SVR, {'kernel': ['linear', 'rbf'], \"C\": [1e-3, 1e-4, 1e-5, 1e-7], \"gamma\": [\"scale\"]})\n",
    "    'SVR': (SVR, {'kernel': ['linear',], \"C\": [1e-3, 1e-5, 1e-7], \"gamma\": [\"scale\"]})\n",
    "}\n",
    "len(enumerate_models(regression_models))\n",
    "\n",
    "\n",
    "X = {'classification': X_classification}#'tfidf': X_tfidf, 'w2v':X_w2v, 'bert1': bert1_embeddings, 'bert2': bert2_embeddings}\n",
    "Y_st = Y[:, 0]\n",
    "Y_lt = Y[:, 1]\n",
    "\n",
    "\n",
    "folds = {}\n",
    "print('Short term memorability prediction:'.upper())\n",
    "\n",
    "for k in X:\n",
    "    folds[k] = {}\n",
    "    print('\\nFeatures:', k.upper(), '\\n')\n",
    "    for regressor in enumerate_models(regression_models):\n",
    "        model_name = str(regressor)\n",
    "        folds[k][model_name] = []\n",
    "        kf = KFold(n_splits=6)#, random_state=42)\n",
    "        print('Training', model_name, '..')\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(X[k])):\n",
    "            print('Fold #'+ str(i), end='.. ')\n",
    "            t = time.time()\n",
    "            \n",
    "            X_train, X_test = X[k][train_index], X[k][test_index]\n",
    "            y_train, y_test = Y_st[train_index], Y_st[test_index]\n",
    "            regressor.fit(X_train, y_train)\n",
    "            #important_tokens = pd.DataFrame(data=regressor.coef_[0],index=vectorizer.get_feature_names(),\n",
    "            #columns=['coefficient']).sort_values(by='coefficient',ascending=False)\n",
    "            #print(important_tokens[0:30])\n",
    "            #print(important_tokens[-30:])\n",
    "            y_pred = regressor.predict(X_test)\n",
    "            folds[k][model_name].append((y_pred, y_test))\n",
    "            print(f'done! ({(time.time() - t):.2} secs). Spearman: {spearman(y_pred, y_test):.2}')\n",
    "            \n",
    "            t = time.time()\n",
    "\n",
    "\n",
    "folds_lt = {}\n",
    "print('Long term memorability prediction:'.upper())\n",
    "\n",
    "for k in X:\n",
    "    folds_lt[k] = {}\n",
    "    print('\\nFeatures:', k.upper(), '\\n')\n",
    "    for regressor in enumerate_models(regression_models):\n",
    "        model_name = str(regressor)\n",
    "        folds_lt[k][model_name] = []\n",
    "        kf = KFold(n_splits=6)#, random_state=42)\n",
    "        print('Training', model_name, '..')\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(X[k])):\n",
    "            print('Fold #'+ str(i), end='.. ')\n",
    "            t = time.time()\n",
    "            X_train, X_test = X[k][train_index], X[k][test_index]\n",
    "            y_train, y_test = Y_lt[train_index], Y_lt[test_index]\n",
    "            regressor.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = regressor.predict(X_test)\n",
    "            folds_lt[k][model_name].append((y_pred, y_test))\n",
    "            print(f'done! ({(time.time() - t):.2} secs). Spearman: {spearman(y_pred, y_test):.2}')\n",
    "\n",
    "            \n",
    "            t = time.time()\n",
    "\n",
    "            \n",
    "for term, all_folds in [('Short term', folds), ('Long term', folds_lt), ('Long Short term', folds_lt)]:\n",
    "    print(term.upper())\n",
    "    for embedding in all_folds:\n",
    "        print('  USING', embedding)\n",
    "        for i, model_name in enumerate(all_folds[embedding]):\n",
    "            # print('    ', (model_name.split('(')[0] + ' ' + str(i+1)).ljust(18), '\\t', end=' ')\n",
    "            print('    ', model_name, '\\t', end=' ')\n",
    "            # print(', '.join([str(spearman(y_p, y_t)) for y_p, y_t in all_folds[embedding][model_name]]))\n",
    "            if term == 'Long Short term':\n",
    "                sps = [spearman(ys_p, yl_t) for (ys_p, ys_t), (yl_p, yl_t)\n",
    "                       in zip(folds[embedding][model_name], folds_lt[embedding][model_name])]\n",
    "            else:\n",
    "                sps = [spearman(y_p, y_t) for y_p, y_t in all_folds[embedding][model_name]]\n",
    "            print(round(sum(sps)/len(sps), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USING classification\n",
    "     SVR(C=0.001, kernel='linear') \t 0.2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman           0.027050\n",
    "man             0.021254\n",
    "girl            0.009931\n",
    "using           0.009736\n",
    "black           0.009590\n",
    "baby            0.009285\n",
    "hand            0.008833\n",
    "piece           0.008495\n",
    "cat             0.008259\n",
    "shirt           0.008093\n",
    "machine         0.007570\n",
    "dog             0.007490\n",
    "red             0.007462\n",
    "weight          0.007433\n",
    "dress           0.006908\n",
    "lady            0.006811\n",
    "food            0.006685\n",
    "smile           0.006132\n",
    "hold            0.005869\n",
    "hair            0.005764\n",
    "push            0.005683\n",
    "young           0.005341\n",
    "brown           0.005287\n",
    "metal           0.005124\n",
    "toy             0.005054\n",
    "lift            0.005040\n",
    "man wearing     0.004771\n",
    "white           0.004735\n",
    "pink            0.004698\n",
    "sitting         0.004674\n",
    "done! (9.3e+01 secs). Spearman: 0.46\n",
    "            coefficient\n",
    "woman       0.026767\n",
    "man         0.017613\n",
    "black       0.010871\n",
    "using       0.010502\n",
    "piece       0.009759\n",
    "cat         0.008430\n",
    "girl        0.008265\n",
    "hand        0.007965\n",
    "dress       0.007930\n",
    "lady        0.007490\n",
    "weight      0.007433\n",
    "baby        0.006793\n",
    "smile       0.006459\n",
    "wooden      0.006425\n",
    "shirt       0.006305\n",
    "dog         0.006287\n",
    "machine     0.006039\n",
    "hair        0.005767\n",
    "lift        0.005672\n",
    "toy         0.005340\n",
    "push        0.005177\n",
    "head        0.005082\n",
    "blonde      0.005049\n",
    "metal       0.004991\n",
    "roll        0.004873\n",
    "brown       0.004841\n",
    "doll        0.004829\n",
    "hold        0.004595\n",
    "young       0.004549\n",
    "box         0.004546\n",
    "done! (9.6e+01 secs). Spearman: 0.54\n",
    "    \n",
    "    snow         -0.008271\n",
    "pan          -0.008348\n",
    "player       -0.008407\n",
    "soccer       -0.008594\n",
    "skiing       -0.008766\n",
    "ocean        -0.008870\n",
    "large        -0.008934\n",
    "band         -0.008980\n",
    "football     -0.009054\n",
    "crowd        -0.009117\n",
    "driving      -0.009170\n",
    "ride         -0.009203\n",
    "stage        -0.009378\n",
    "road         -0.009425\n",
    "race         -0.009509\n",
    "street       -0.009620\n",
    "waterfall    -0.009799\n",
    "rock         -0.009804\n",
    "dark         -0.009942\n",
    "field        -0.010205\n",
    "wave         -0.011475\n",
    "hill         -0.011821\n",
    "snowy        -0.012301\n",
    "ski          -0.012648\n",
    "view         -0.013653\n",
    "car          -0.016878\n",
    "tree         -0.016965\n",
    "mountain     -0.019070\n",
    "group        -0.022045\n",
    "people       -0.038964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEASURING PERXPLEXITY\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "device = 'cuda'\n",
    "model_id = 'gpt2-large'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
