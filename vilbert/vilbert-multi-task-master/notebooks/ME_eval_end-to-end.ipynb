{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/tfutils/optimizer.py:18: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from io import open\n",
    "import numpy as np\n",
    "import math\n",
    "import _pickle as cPickle\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from bisect import bisect\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from vilbert.task_utils import (\n",
    "    LoadDatasetEval,\n",
    "    LoadLosses,\n",
    "    ForwardModelsTrain,\n",
    "    ForwardModelsVal,\n",
    "    EvaluatingModel,\n",
    ")\n",
    "\n",
    "import vilbert.utils as utils\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--task_specific_tokens'], dest='task_specific_tokens', nargs=0, const=True, default=False, type=None, choices=None, help='whether to use task specific tokens for the multi-task learning.', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--bert_model\",\n",
    "    default=\"bert-base-uncased\",\n",
    "    type=str,\n",
    "    help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "    \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--from_pretrained\",\n",
    "    default=\"bert-base-uncased\",\n",
    "    type=str,\n",
    "    help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "    \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    default=\"results\",\n",
    "    type=str,\n",
    "    help=\"The output directory where the model checkpoints will be written.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config_file\",\n",
    "    default=\"config/bert_config.json\",\n",
    "    type=str,\n",
    "    help=\"The config file which specified the model details.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_lower_case\",\n",
    "    default=True,\n",
    "    type=bool,\n",
    "    help=\"Whether to lower case the input text. True for uncased models, False for cased models.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local_rank\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"local_rank for distributed training on gpus\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=42, help=\"random seed for initialization\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fp16\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to use 16-bit float precision instead of 32-bit\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loss_scale\",\n",
    "    type=float,\n",
    "    default=0,\n",
    "    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "    \"0 (default value): dynamic loss scaling.\\n\"\n",
    "    \"Positive power of 2: static loss scaling value.\\n\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\",\n",
    "    type=int,\n",
    "    default=16,\n",
    "    help=\"Number of workers in the dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_name\", default=\"\", type=str, help=\"save name for training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_chunk\",\n",
    "    default=0,\n",
    "    type=float,\n",
    "    help=\"whether use chunck for parallel training.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", default=30, type=int, help=\"what is the batch size?\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tasks\", default=\"\", type=str, help=\"1-2-3... training task separate by -\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--in_memory\",\n",
    "    default=False,\n",
    "    type=bool,\n",
    "    help=\"whether use chunck for parallel training.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--baseline\", action=\"store_true\", help=\"whether use single stream baseline.\"\n",
    ")\n",
    "parser.add_argument(\"--split\", default=\"\", type=str, help=\"which split to use.\")\n",
    "parser.add_argument(\n",
    "    \"--dynamic_attention\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether use dynamic attention.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clean_train_sets\",\n",
    "    default=True,\n",
    "    type=bool,\n",
    "    help=\"whether clean train sets for multitask data.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visual_target\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"which target to use for visual branch. \\\n",
    "    0: soft label, \\\n",
    "    1: regress the feature, \\\n",
    "    2: NCE loss.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--task_specific_tokens\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether to use task specific tokens for the multi-task learning.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the textual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--bert_model', 'bert-base-uncased',\n",
    "                          '--from_pretrained', 'save/ME_bert_base_6layer_6conect-finetune_from_multi_task_model-task_19-all_train-NLVR2/pytorch_model_11.bin',\n",
    "                          '--config_file', 'config/bert_base_6layer_6conect.json',\n",
    "                          '--tasks', '19',\n",
    "                          '--split', 'test',\n",
    "                          '--save_name', 'task-19-end-to-end',\n",
    "                          '--task_specific_tokens',\n",
    "                          '--batch_size', '128'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/28/2020 12:13:11 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/28/2020 12:13:12 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/aloui/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "08/28/2020 12:13:12 - INFO - vilbert.task_utils -   Loading ME Dataset with batch size 128\n",
      "08/28/2020 12:13:12 - INFO - vilbert.datasets.me_dataset -   Loading from datasets/ME/cache/ME_test_23_cleaned.pkl\n",
      "08/28/2020 12:13:12 - INFO - vilbert.utils -   logging file at: pytorch_model_10.bin-task-19-end-to-end\n"
     ]
    }
   ],
   "source": [
    "with open(\"vilbert_tasks.yml\", \"r\") as f:\n",
    "    task_cfg = edict(yaml.safe_load(f))\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.baseline:\n",
    "    from pytorch_transformers.modeling_bert import BertConfig\n",
    "    from vilbert.basebert import BaseBertForVLTasks\n",
    "else:\n",
    "    from vilbert.vilbert import BertConfig\n",
    "    from vilbert.vilbert import VILBertForVLTasks\n",
    "\n",
    "task_names = []\n",
    "for i, task_id in enumerate(args.tasks.split(\"-\")):\n",
    "    task = \"TASK\" + task_id\n",
    "    name = task_cfg[task][\"name\"]\n",
    "    task_names.append(name)\n",
    "\n",
    "# timeStamp = '-'.join(task_names) + '_' + args.config_file.split('/')[1].split('.')[0]\n",
    "timeStamp = args.from_pretrained.split(\"/\")[-1] + \"-\" + args.save_name\n",
    "savePath = os.path.join(args.output_dir, timeStamp)\n",
    "config = BertConfig.from_json_file(args.config_file)\n",
    "\n",
    "if args.task_specific_tokens:\n",
    "    config.task_specific_tokens = True\n",
    "\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "    )\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "\n",
    "logger.info(\n",
    "    \"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16\n",
    "    )\n",
    ")\n",
    "\n",
    "default_gpu = False\n",
    "if dist.is_available() and args.local_rank != -1:\n",
    "    rank = dist.get_rank()\n",
    "    if rank == 0:\n",
    "        default_gpu = True\n",
    "else:\n",
    "    default_gpu = True\n",
    "\n",
    "if default_gpu and not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "\n",
    "task_batch_size, task_num_iters, task_ids, task_datasets_val, task_dataloader_val = LoadDatasetEval(\n",
    "    args, task_cfg, args.tasks.split(\"-\")\n",
    ")\n",
    "\n",
    "tbLogger = utils.tbLogger(\n",
    "    timeStamp,\n",
    "    savePath,\n",
    "    task_names,\n",
    "    task_ids,\n",
    "    task_num_iters,\n",
    "    1,\n",
    "    save_logger=False,\n",
    "    txt_name=\"eval.txt\",\n",
    ")\n",
    "# num_labels = max([dataset.num_labels for dataset in task_datasets_val.values()])\n",
    "\n",
    "if args.dynamic_attention:\n",
    "    config.dynamic_attention = True\n",
    "if \"roberta\" in args.bert_model:\n",
    "    config.model = \"roberta\"\n",
    "\n",
    "if args.visual_target == 0:\n",
    "    config.v_target_size = 1601\n",
    "    config.visual_target = args.visual_target\n",
    "else:\n",
    "    config.v_target_size = 2048\n",
    "    config.visual_target = args.visual_target\n",
    "\n",
    "if args.task_specific_tokens:\n",
    "    config.task_specific_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'TASK19': 128},\n",
       " {'TASK19': 16},\n",
       " ['TASK19'],\n",
       " {'TASK19': <vilbert.datasets.me_dataset.MERegressionDataset at 0x7f2090ad8940>},\n",
       " {'TASK19': <torch.utils.data.dataloader.DataLoader at 0x7f2096ed3160>})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_batch_size, task_num_iters, task_ids, task_datasets_val, task_dataloader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(task_datasets_val['TASK19']), len(task_dataloader_val['TASK19'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/28/2020 12:13:14 - INFO - vilbert.utils -   loading weights file save/ME_bert_base_6layer_6conect-finetune_from_multi_task_model-task_19-all_train-NLVR2/pytorch_model_10.bin\n"
     ]
    }
   ],
   "source": [
    "num_labels = 2\n",
    "\n",
    "if args.baseline:\n",
    "    model = BaseBertForVLTasks.from_pretrained(\n",
    "        args.from_pretrained,\n",
    "        config=config,\n",
    "        num_labels=num_labels,\n",
    "        default_gpu=default_gpu,\n",
    "    )\n",
    "else:\n",
    "    model = VILBertForVLTasks.from_pretrained(\n",
    "        args.from_pretrained,\n",
    "        config=config,\n",
    "        num_labels=num_labels,\n",
    "        default_gpu=default_gpu,\n",
    "    )\n",
    "\n",
    "task_losses = LoadLosses(args, task_cfg, args.tasks.split(\"-\"))\n",
    "model.to(device)\n",
    "if args.local_rank != -1:\n",
    "    try:\n",
    "        from apex.parallel import DistributedDataParallel as DDP\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"\n",
    "        )\n",
    "    model = DDP(model, delay_allreduce=True)\n",
    "\n",
    "elif n_gpu > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagate Training Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num Iters:  {'TASK19': 16}\n",
      "  Batch size:  {'TASK19': 128}\n",
      "15/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/28/2020 12:13:30 - INFO - vilbert.utils -   Eval task TASK19 on iteration 0 \n",
      "08/28/2020 12:13:30 - INFO - vilbert.utils -   Validation [ME]: loss 0.026 score 0.381 \n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num Iters: \", task_num_iters)\n",
    "print(\"  Batch size: \", task_batch_size)\n",
    "\n",
    "targets_list, preds_list = list(), list()\n",
    "\n",
    "model.eval()\n",
    "# when run evaluate, we run each task sequentially.\n",
    "for task_id in task_ids:\n",
    "    results = []\n",
    "    others = []\n",
    "    for i, batch in enumerate(task_dataloader_val[task_id]):\n",
    "        loss, score, batch_size, results, others, vil_score_prediction = EvaluatingModel(\n",
    "            args,\n",
    "            task_cfg,\n",
    "            device,\n",
    "            task_id,\n",
    "            batch,\n",
    "            model,\n",
    "            task_dataloader_val,\n",
    "            task_losses,\n",
    "            results,\n",
    "            others,\n",
    "        )\n",
    "        targets_list.append(batch[4]) # target\n",
    "        preds_list.append(vil_score_prediction)\n",
    "        \n",
    "        tbLogger.step_val(0, float(loss), float(score), task_id, batch_size, \"val\")\n",
    "        sys.stdout.write(\"%d/%d\\r\" % (i, len(task_dataloader_val[task_id])))\n",
    "        sys.stdout.flush()\n",
    "    # save the result or evaluate the result.\n",
    "    ave_score = tbLogger.showLossVal(task_id)\n",
    "\n",
    "    if args.split:\n",
    "        json_path = os.path.join(savePath, args.split)\n",
    "    else:\n",
    "        json_path = os.path.join(savePath, task_cfg[task_id][\"val_split\"])\n",
    "\n",
    "    json.dump(results, open(json_path + \"_result.json\", \"w\"))\n",
    "    json.dump(others, open(json_path + \"_others.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets_list), len(preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.cat(targets_list, 0)\n",
    "preds = torch.cat(preds_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4771996524236738"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, _ = spearmanr(\n",
    "    targets.cpu().detach().numpy()[:,0], \n",
    "    preds.cpu().detach().numpy()[:,0], \n",
    "    axis=0\n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22844158655840108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, _ = spearmanr(\n",
    "    targets.cpu().detach().numpy()[:,1], \n",
    "    preds.cpu().detach().numpy()[:,1], \n",
    "    axis=0\n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
